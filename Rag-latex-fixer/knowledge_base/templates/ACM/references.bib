@book{lamport94,
 author = "Leslie Lamport",
 title = "{\LaTeX: A Document Preparation System}",
 year = "1994",
 publisher = "Addison-Wesley",
 edition = "2nd",
 address = "Reading, Massachusetts"
}

@inproceedings{nicepaper1,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2",
  title = "A Very Nice Paper To Cite",
  year = "2016",
  booktitle = "Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture"
}

@article{autodnnchip,
  author       = {Pengfei Xu and
                  Xiaofan Zhang and
                  Cong Hao and
                  Yang Zhao and
                  Yongan Zhang and
                  Yue Wang and
                  Chaojian Li and
                  Zetong Guan and
                  Deming Chen and
                  Yingyan Lin},
  title        = {AutoDNNchip: An Automated {DNN} Chip Predictor and Builder for Both
                  FPGAs and ASICs},
  journal      = {CoRR},
  volume       = {abs/2001.03535},
  year         = {2020},
  url          = {https://arxiv.org/abs/2001.03535},
  eprinttype    = {arXiv},
  eprint       = {2001.03535},
  timestamp    = {Tue, 23 Aug 2022 09:01:21 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2001-03535.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{sheng2023flexgen,
  title={FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU},
  author={Sheng and others},
  year={2023}
}

@inproceedings{nicepaper2,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3",
  title = "Another Very Nice Paper to Cite",
  year = "2015",
  booktitle = "Proceedings of the 48th Annual IEEE/ACM International Symposium on Microarchitecture"
}

@inproceedings{nicepaper3,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3 and Firstname4 Lastname4 and Firstname5 Lastname5 and Firstname6 Lastname6 and Firstname7 Lastname7 and Firstname8 Lastname8 and Firstname9 Lastname9 and Firstname10 Lastname10 and Firstname11 Lastname11 and Firstname12 Lastname12",
  title = "Yet Another Very Nice Paper To Cite, With Many Author Names All Spelled Out",
  year = "2011",
  booktitle = "Proceedings of the 38th Annual International Symposium on Computer Architecture"
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  volume={30},
  year={2017}
}


@article{yurtsever2020survey,
  title={A survey of autonomous driving: Common practices and emerging technologies},
  author={Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
  journal={IEEE access},
  volume={8},
  pages={58443--58469},
  year={2020},
  publisher={IEEE}
}



@misc{hlx,
author = {{Xilinx Inc.}},
title = {Vivado Design Suite},
howpublished = {\url{https://www.xilinx.com/products/design-tools/vivado.html}},
month = {},
year = {},
note = {(Accessed on 05/08/2023)}
}
@inproceedings{TPU,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and others},
  booktitle={2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)},
  pages={1--12},
  year={2017},
  organization={IEEE}
}

@inproceedings{fu2020autogandistiller,
      title={{A}utoGAN-{D}istiller: Searching to Compress Generative Adversarial Networks}, 
      author={Fu, Y. and others},
      booktitle = {ICML'20},
}

@inproceedings{you2020shiftaddnet,
 author = {You, Haoran and Chen, Xiaohan and Zhang, Yongan and Li, Chaojian and Li, Sicheng and Liu, Zihao and Wang, Zhangyang and Lin, Yingyan},
 booktitle = {NeurIPS},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {2771--2783},
 publisher = {Curran Associates, Inc.},
 title = {ShiftAddNet: A Hardware-Inspired Deep Network},
 url = {https://proceedings.neurips.cc/paper/2020/file/1cf44d7975e6c86cffa70cae95b5fbb2-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{pmlr-v80-wu18h,
  title = 	 {Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions},
  author =       {Wu, Junru and Wang, Yue and Wu, Zhenyu and Wang, Zhangyang and Veeraraghavan, Ashok and Lin, Yingyan},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5363--5372},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/wu18h/wu18h.pdf},
  url = 	 {http://proceedings.mlr.press/v80/wu18h.html},
  abstract = 	 {The current trend of pushing CNNs deeper with convolutions has created a pressing demand to achieve higher compression gains on CNNs where convolutions dominate the computation and parameter amount (e.g., GoogLeNet, ResNet and Wide ResNet). Further, the high energy consumption of convolutions limits its deployment on mobile devices. To this end, we proposed a simple yet effective scheme for compressing convolutions though applying k-means clustering on the weights, compression is achieved through weight-sharing, by only recording $K$ cluster centers and weight assignment indexes. We then introduced a novel spectrally relaxed $k$-means regularization, which tends to make hard assignments of convolutional layer weights to $K$ learned cluster centers during re-training. We additionally propose an improved set of metrics to estimate energy consumption of CNN hardware implementations, whose estimation results are verified to be consistent with previously proposed energy estimation tool extrapolated from actual hardware measurements. We finally evaluated Deep $k$-Means across several CNN models in terms of both compression ratio and energy consumption reduction, observing promising results without incurring accuracy loss. The code is available at https://github.com/Sandbox3aster/Deep-K-Means}
}


@inproceedings{10.1145/3210240.3210337,
author = {Liu, Sicong and Lin, Yingyan and Zhou, Zimu and Nan, Kaiming and Liu, Hui and Du, Junzhao},
title = {On-Demand Deep Model Compression for Mobile Devices: A Usage-Driven Model Selection Framework},
year = {2018},
isbn = {9781450357203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210240.3210337},
doi = {10.1145/3210240.3210337},
abstract = {Recent research has demonstrated the potential of deploying deep neural networks (DNNs) on resource-constrained mobile platforms by trimming down the network complexity using different compression techniques. The current practice only investigate stand-alone compression schemes even though each compression technique may be well suited only for certain types of DNN layers. Also, these compression techniques are optimized merely for the inference accuracy of DNNs, without explicitly considering other application-driven system performance (e.g. latency and energy cost) and the varying resource availabilities across platforms (e.g. storage and processing capability). In this paper, we explore the desirable tradeoff between performance and resource constraints by user-specified needs, from a holistic system-level viewpoint. Specifically, we develop a usage-driven selection framework, referred to as AdaDeep, to automatically select a combination of compression techniques for a given DNN, that will lead to an optimal balance between user-specified performance goals and resource constraints. With an extensive evaluation on five public datasets and across twelve mobile devices, experimental results show that AdaDeep enables up to 9.8x latency reduction, 4.3x energy efficiency improvement, and 38x storage reduction in DNNs while incurring negligible accuracy loss. AdaDeep also uncovers multiple effective combinations of compression techniques unexplored in existing literature.},
booktitle = {Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services},
pages = {389–400},
numpages = {12},
keywords = {deep learning, deep reinforcement learning, model compression},
location = {Munich, Germany},
series = {MobiSys '18}
}


@article{dna,
  author    = {Yongan Zhang and
               Yonggan Fu and
               Weiwen Jiang and
               Chaojian Li and
               Haoran You and
               Meng Li and
               Vikas Chandra and
               Yingyan Lin},
  title     = {{DNA:} Differentiable Network-Accelerator Co-Search},
  journal   = {CoRR},
  volume    = {abs/2010.14778},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.14778},
  archivePrefix = {arXiv},
  eprint    = {2010.14778},
  timestamp = {Mon, 02 Nov 2020 18:47:10 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-14778.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Xu_2020,
  title={{A}uto{DNN}chip: An Automated DNN Chip Predictor and Builder for Both {FPGA}s and {ASIC}s},
   ISBN={9781450370998},
   url={http://dx.doi.org/10.1145/3373087.3375306},
   DOI={10.1145/3373087.3375306},
   journal={The 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
   publisher={ACM},
   author={Xu, Pengfei and Zhang, Xiaofan and Hao, Cong and Zhao, Yang and Zhang, Yongan and Wang, Yue and Li, Chaojian and Guan, Zetong and Chen, Deming and Lin, Yingyan},
   year={2020},
   month={Feb}
}


@INPROCEEDINGS{9138981,
  author={Zhao, Yang and Chen, Xiaohan and Wang, Yue and Li, Chaojian and You, Haoran and Fu, Yonggan and Xie, Yuan and Wang, Zhangyang and Lin, Yingyan},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={{S}mart{E}xchange: Trading Higher-cost Memory Storage/Access for Lower-cost Computation}, 
  year={2020},
  volume={},
  number={},
  pages={954-967},
  doi={10.1109/ISCA45697.2020.00082}}

@INPROCEEDINGS{9138916,
  author={Li, Weitao and Xu, Pengfei and Zhao, Yang and Li, Haitong and Xie, Yuan and Lin, Yingyan},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Timely: Pushing Data Movements And Interfaces In Pim Accelerators Towards Local And In Time Domain}, 
  year={2020},
  volume={},
  number={},
  pages={832-845},
  doi={10.1109/ISCA45697.2020.00073}}

@inproceedings{qin2020sigma,
  title={Sigma: A sparse and irregular gemm accelerator with flexible interconnects for dnn training},
  author={Qin, Eric and Samajdar, Ananda and Kwon, Hyoukjun and Nadella, Vineet and Srinivasan, Sudarshan and Das, Dipankar and Kaul, Bharat and Krishna, Tushar},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={58--70},
  year={2020},
  organization={IEEE}
}

@article{cacti7,
author = {Balasubramonian, Rajeev and Kahng, Andrew B. and Muralimanohar, Naveen and Shafiee, Ali and Srinivas, Vaishnav},
title = {CACTI 7: New Tools for Interconnect Exploration in Innovative Off-Chip Memories},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1544-3566},
url = {https://doi.org/10.1145/3085572},
doi = {10.1145/3085572},
journal = {ACM Trans. Archit. Code Optim.},
month = jun,
articleno = {14},
numpages = {25},
keywords = {tools, DRAM, Memory, NVM, interconnects}
}

@INPROCEEDINGS{timeloop,
  author={A. {Parashar} and P. {Raina} and Y. S. {Shao} and Y. {Chen} and V. A. {Ying} and A. {Mukkara} and R. {Venkatesan} and B. {Khailany} and S. W. {Keckler} and J. {Emer}},
  booktitle={2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)}, 
  title={Timeloop: A Systematic Approach to DNN Accelerator Evaluation}, 
  year={2019},
  volume={},
  number={},
  pages={304-315},}

@INPROCEEDINGS{accelergy,
  author={Y. N. {Wu} and J. S. {Emer} and V. {Sze}},
  booktitle={2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
  title={Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs}, 
  year={2019},
  volume={},
  number={},
  pages={1-8},}
  
  
@INPROCEEDINGS{aladdin,
  author={Y. S. {Shao} and B. {Reagen} and G. {Wei} and D. {Brooks}},
  booktitle={2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)}, 
  title={Aladdin: A pre-RTL, power-performance accelerator simulator enabling large design space exploration of customized architectures}, 
  year={2014},
  volume={},
  number={},
  pages={97-108},}
@article{chen2018joint,
  title={Joint neural architecture search and quantization},
  author={Chen, Yukang and Meng, Gaofeng and Zhang, Qian and Zhang, Xinbang and Song, Liangchen and Xiang, Shiming and Pan, Chunhong},
  journal={arXiv preprint arXiv:1811.09426},
  year={2018}
}

@inproceedings{dnnbuilder,
author = {Zhang, Xiaofan and Wang, Junsong and Zhu, Chao and Lin, Yonghua and Xiong, Jinjun and Hwu, Wen-mei and Chen, Deming},
title = {DNNBuilder: An Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs},
year = {2018},
isbn = {9781450359504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240765.3240801},
doi = {10.1145/3240765.3240801},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
articleno = {56},
numpages = {8},
location = {San Diego, California},
series = {ICCAD '18}
}

@inproceedings{shen2017,
author = {Shen, Yongming and Ferdman, Michael and Milder, Peter},
title = {Maximizing CNN Accelerator Efficiency Through Resource Partitioning},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080221},
doi = {10.1145/3079856.3080221},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {535–547},
numpages = {13},
keywords = {FPGA, Convolutional Neural Network, Accelerator},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}

@inproceedings{zhang2015,
author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
title = {Optimizing FPGA-Based Accelerator Design for Deep Convolutional Neural Networks},
year = {2015},
isbn = {9781450333153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2684746.2689060},
doi = {10.1145/2684746.2689060},
booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {161–170},
numpages = {10},
keywords = {convolutional neural network, roofline model, acceleration, fpga},
location = {Monterey, California, USA},
series = {FPGA '15}
}

@misc{yang2016systematic,
      title={A Systematic Approach to Blocking Convolutional Neural Networks}, 
      author={Xuan Yang and Jing Pu and Blaine Burton Rister and Nikhil Bhagdikar and Stephen Richardson and Shahar Kvatinsky and Jonathan Ragan-Kelley and Ardavan Pedram and Mark Horowitz},
      year={2016},
      eprint={1606.04209},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@inproceedings{gong2019mixed,
  title={Mixed Precision Neural Architecture Search for Energy Efficient Deep Learning.},
  author={Gong, Chengyue and Jiang, Zixuan and Wang, Dilin and Lin, Yibo and Liu, Qiang and Pan, David Z},
  booktitle={ICCAD},
  pages={1--7},
  year={2019}
}

@inproceedings{wang2020apq,
  title={APQ: Joint Search for Network Architecture, Pruning and Quantization Policy},
  author={Wang, Tianzhe and Wang, Kuan and Cai, Han and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Lin, Yujun and Han, Song},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2078--2087},
  year={2020}
}


@inproceedings{hao2019fpga,
  title={FPGA/DNN Co-Design: An Efficient Design Methodology for 1oT Intelligence on the Edge},
  author={Hao, Cong and Zhang, Xiaofan and Li, Yuhong and Huang, Sitao and Xiong, Jinjun and Rupnow, Kyle and Hwu, Wen-mei and Chen, Deming},
  booktitle={2019 56th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2019},
  organization={IEEE}
}

@article{abdelfattah2020best,
  title={Best of Both Worlds: AutoML Codesign of a CNN and its Hardware Accelerator},
  author={Abdelfattah, Mohamed S and Dudziak, {\L}ukasz and Chau, Thomas and Lee, Royson and Kim, Hyeji and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2002.05022},
  year={2020}
}

@article{yang2020co,
  title={Co-Exploration of Neural Architectures and Heterogeneous ASIC Accelerator Designs Targeting Multiple Tasks},
  author={Yang, Lei and Yan, Zheyu and Li, Meng and Kwon, Hyoukjun and Lai, Liangzhen and Krishna, Tushar and Chandra, Vikas and Jiang, Weiwen and Shi, Yiyu},
  journal={arXiv preprint arXiv:2002.04116},
  year={2020}
}

@article{jiang2020device,
  title={Device-circuit-architecture co-exploration for computing-in-memory neural accelerators},
  author={Jiang, Weiwen and Lou, Qiuwen and Yan, Zheyu and Yang, Lei and Hu, Jingtong and Hu, X Sharon and Shi, Yiyu},
  journal={IEEE Transactions on Computers},
  year={2020},
  publisher={IEEE}
}

@article{jiang2020hardware,
  title={Hardware/Software co-exploration of neural architectures},
  author={Jiang, Weiwen and Yang, Lei and Sha, Edwin H-M and Zhuge, Qingfeng and Gu, Shouzhen and Dasgupta, Sakyasingha and Shi, Yiyu and Hu, Jingtong},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year={2020},
  publisher={IEEE}
}


@article{li2020edd,
  title={EDD: Efficient Differentiable DNN Architecture and Implementation Co-search for Embedded AI Solutions},
  author={Li, Yuhong and Hao, Cong and Zhang, Xiaofan and Liu, Xinheng and Chen, Yao and Xiong, Jinjun and Hwu, Wen-mei and Chen, Deming},
  journal={arXiv preprint arXiv:2005.02563},
  year={2020}
}




@inproceedings{jin2020adabits,
  title={Adabits: Neural network quantization with adaptive bit-widths},
  author={Jin, Qing and Yang, Linjie and Liao, Zhenyu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2146--2156},
  year={2020}
}


@inproceedings{tan2019mnasnet,
  title={Mnasnet: Platform-aware neural architecture search for mobile},
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2820--2828},
  year={2019}
}


@inproceedings{howard2019searching,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and and others},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1314--1324},
  year={2019}
}


@article{tan2019efficientnet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc V},
  journal={arXiv preprint arXiv:1905.11946},
  year={2019}
}

@article{liu2018darts,
  title={Darts: Differentiable architecture search},
  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  journal={arXiv preprint arXiv:1806.09055},
  year={2018}
}

@inproceedings{maddison2014sampling,
  title={A* sampling},
  author={Maddison, Chris J and Tarlow, Daniel and Minka, Tom},
  booktitle={NeurIPS},
  year={2014}
}

@inproceedings{wu2019fbnet,
  title={Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search},
  author={Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={10734--10742},
  year={2019}
}

@inproceedings{wan2020fbnetv2,
  title={Fbnetv2: Differentiable neural architecture search for spatial and channel dimensions},
  author={Wan, Alvin and Dai, Xiaoliang and Zhang, Peizhao and He, Zijian and Tian, Yuandong and Xie, Saining and Wu, Bichen and Yu, Matthew and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12965--12974},
  year={2020}
}

@article{cai2018proxylessnas,
  title={Proxylessnas: Direct neural architecture search on target task and hardware},
  author={Cai, Han and Zhu, Ligeng and Han, Song},
  journal={arXiv preprint arXiv:1812.00332},
  year={2018}
}

@inproceedings{stamoulis2019single,
  title={Single-path nas: Designing hardware-efficient convnets in less than 4 hours},
  author={Stamoulis, Dimitrios and Ding, Ruizhou and Wang, Di and Lymberopoulos, Dimitrios and Priyantha, Bodhi and Liu, Jie and Marculescu, Diana},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={481--497},
  year={2019},
  organization={Springer}
}


@inproceedings{he2018amc,
  title={Amc: Automl for model compression and acceleration on mobile devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={784--800},
  year={2018}
}

@inproceedings{wang2019haq,
  title={Haq: Hardware-aware automated quantization with mixed precision},
  author={Wang, Kuan and Liu, Zhijian and Lin, Yujun and Lin, Ji and Han, Song},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8612--8620},
  year={2019}
}

@article{wu2018mixed,
  title={Mixed precision quantization of convnets via differentiable neural architecture search},
  author={Wu, Bichen and Wang, Yanghan and Zhang, Peizhao and Tian, Yuandong and Vajda, Peter and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1812.00090},
  year={2018}
}

@inproceedings{cai2020rethinking,
  title={Rethinking Differentiable Search for Mixed-Precision Neural Networks},
  author={Cai, Zhaowei and Vasconcelos, Nuno},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2349--2358},
  year={2020}
}




@misc{zhao2020smartexchange,
      title={SmartExchange: Trading Higher-cost Memory Storage/Access for Lower-cost Computation}, 
      author={Yang Zhao and Xiaohan Chen and Yue Wang and Chaojian Li and Haoran You and Yonggan Fu and Yuan Xie and Zhangyang Wang and Yingyan Lin},
      year={2020},
      eprint={2005.03403},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{elthakeb2020releq,
  title={ReLeQ: A Reinforcement Learning Approach for Automatic Deep Quantization of Neural Networks},
  author={Elthakeb, Ahmed Taha and Pilligundla, Prannoy and Mireshghallah, Fatemeh and Yazdanbakhsh, Amir and Esmaeilzadeh, Hadi},
  journal={IEEE Micro},
  year={2020},
  publisher={IEEE}
}

@inproceedings{venkatesanmagnet,
  title={{MAGNet: A Modular Accelerator Generator for Neural Networks}},
  author={Venkatesan, Rangharajan and Shao, Yakun Sophia and Wang, Miaorong and Clemons, Jason and Dai, Steve and Fojtik, Matthew and others},
  booktitle={Proceedings of the International Conference on Computer-Aided Design (ICCAD)},
  year={2019},
}


@inproceedings{wang2018design,
  title={{Design flow of accelerating hybrid extremely low bit-width neural network in embedded FPGA}},
  author={Wang, Junsong and Lou, Qiuwen and Zhang, Xiaofan and Zhu, Chao and Lin, Yonghua and Chen, Deming},
  booktitle={2018 28th International Conference on Field Programmable Logic and Applications (FPL)},
  year={2018},
}

@article{chen2016eyeriss,
  title={Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks},
  author={Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={367--379},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{chen2017eyeriss,
  title={Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks},
  author={Y. {Chen} and T. {Krishna} and J. {Emer} and V. {Sze}},
  journal={JSSC 2017},
  volume={52},
  number={1},
  pages={127--138},
  year={2017},
  publisher={IEEE}
}
@inproceedings{du2015shidiannao,
  title={ShiDianNao: Shifting vision processing closer to the sensor},
  author={Z. {Du} and R. {Fasthuber} and T. {Chen} and P. {Ienne} and L. {Li} and T. {Luo} and X. {Feng} and Y. {Chen} and O. {Temam}},
  booktitle={ACM SIGARCH Computer Architecture News},
  volume={43},
  number={3},
  pages={92--104},
  year={2015},
  organization={ACM}
}

@misc{zc706,
    author = {{Xilinx Inc.}},
    title = {Xilinx Zynq-7000 SoC ZC706 Evaluation Kit},
    howpublished = {\url{https://www.xilinx.com/products/boards-and-kits/ek-z7-zc706-g.html}},
    month = {},
    year = {},
    note = {(Accessed on 05/08/2023)}
}


@misc{vivado_HLS,
    author = {{Xilinx Inc.}},
    title = {Vitis High-Level Synthesis},
    howpublished = {\url{https://www.xilinx.com/products/design-tools/vitis/vitis-hls.html}},
    month = {},
    year = {},
    note = {(Accessed on 05/08/2023)}
}


@inproceedings{exploring_hetero,
author = {Xiao, Qingcheng and Liang, Yun and Lu, Liqiang and Yan, Shengen and Tai, Yu-Wing},
title = {Exploring Heterogeneous Algorithms for Accelerating Deep Convolutional Neural Networks on FPGAs},
year = {2017},
isbn = {9781450349277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3061639.3062244},
doi = {10.1145/3061639.3062244},
booktitle = {Proceedings of the 54th Annual Design Automation Conference 2017},
articleno = {Article 62},
numpages = {6},
location = {Austin, TX, USA},
series = {DAC ’17}
}


@inproceedings{qiu2016going,
  title={Going deeper with embedded FPGA platform for convolutional neural network},
  author={Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and others},
  booktitle={Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={26--35},
  year={2016},
  organization={ACM}
}


@article{hls_chen2005xpilot,
  title={xpilot: A platform-based behavioral synthesis system},
  author={Chen, Deming and Cong, Jason and Fan, Yiping and Han, Guoling and Jiang, Wei and Zhang, Zhiru},
  journal={SRC TechCon},
  volume={5},
  year={2005}
}

@article{hls_chen2009lopass,
  title={LOPASS: A low-power architectural synthesis system for {FPGAs} with interconnect estimation and optimization},
  author={Chen, Deming and Cong, Jason and Fan, Yiping and Wan, Lu},
  journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume={18},
  number={4},
  pages={564--577},
  year={2009},
  publisher={IEEE}
}

@inproceedings{hls_rupnow2011high,
  title={High level synthesis of stereo matching: Productivity, performance, and software constraints},
  author={Rupnow, Kyle and Liang, Yun and Li, Yinan and Min, Dongbo and Do, Minh and Chen, Deming},
  booktitle={2011 International Conference on Field-Programmable Technology},
  pages={1--8},
  year={2011},
  organization={IEEE}
}

@inproceedings{wang2016deepburning,
author = {Wang, Ying and Xu, Jie and Han, Yinhe and Li, Huawei and Li, Xiaowei},
title = {DeepBurning: Automatic Generation of FPGA-Based Learning Accelerators for the Neural Network Family},
year = {2016},
isbn = {9781450342360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897937.2898003},
doi = {10.1145/2897937.2898003},
articleno = {110},
numpages = {6},
location = {Austin, Texas},
series = {DAC '16}
}



@article{zhang2018caffeine,
  title={{Caffeine: Towards uniformed representation and acceleration for deep convolutional neural networks}},
  author={Zhang, Chen and Sun, Guangyu and Fang, Zhenman and Zhou, Peipei and Pan, Peichen and Cong, Jason},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year={2018},
  publisher={IEEE}
}
@inproceedings{guan2017fp,
  title={{FP-DNN: An automated framework for mapping deep neural networks onto FPGAs with RTL-HLS hybrid templates}},
  author={Guan, Yijin and Liang, Hao and Xu, Ningyi and Wang, Wenqiang and Shi, Shaoshuai and Chen, Xi and Sun, Guangyu and Zhang, Wei and Cong, Jason},
  booktitle={2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
  pages={152--159},
  year={2017},
  organization={IEEE}
}


@article{tian2020discretization,
  title={Discretization-aware architecture search},
  author={Tian, Yunjie and Liu, Chang and Xie, Lingxi and Jiao, Jianbin and Ye, Qixiang},
  journal={arXiv preprint arXiv:2007.03154},
  year={2020}
}


@article{li2019stacnas,
  title={StacNAS: Towards Stable and Consistent Differentiable Neural Architecture Search},
  author={Li, Guilin and Zhang, Xing and Wang, Zitong and Li, Zhenguo and Zhang, Tong},
  journal={arXiv},
  pages={arXiv--1909},
  year={2019}
}


@inproceedings{hong2020dropnas,
  title={Dropnas: Grouped operation dropout for differentiable architecture search},
  author={Hong, Weijun and Li, Guilin and Zhang, Weinan and Tang, Ruiming and Wang, Yunhe and Li, Zhenguo and Yu, Yong},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2020}
}

@inproceedings{dong2019searching,
  title={Searching for a robust neural architecture in four gpu hours},
  author={Dong, Xuanyi and Yang, Yi},
  booktitle={Proceedings of the IEEE Conference on computer vision and pattern recognition},
  pages={1761--1770},
  year={2019}
}

@inproceedings{hu2020dsnas,
  title={DSNAS: Direct Neural Architecture Search without Parameter Retraining},
  author={Hu, Shoukang and Xie, Sirui and Zheng, Hehui and Liu, Chunxiao and Shi, Jianping and Liu, Xunying and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12084--12092},
  year={2020}
}


@article{hu2020tf,
  title={Tf-nas: Rethinking three search freedoms of latency-constrained differentiable neural architecture search},
  author={Hu, Yibo and Wu, Xiang and He, Ran},
  journal={arXiv preprint arXiv:2008.05314},
  year={2020}
}


@article{guerra2020switchable,
  title={Switchable Precision Neural Networks},
  author={Guerra, Luis and Zhuang, Bohan and Reid, Ian and Drummond, Tom},
  journal={arXiv preprint arXiv:2002.02815},
  year={2020}
}


@article{zhou2016dorefa,
  title={Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients},
  author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  journal={arXiv preprint arXiv:1606.06160},
  year={2016}
}


@article{habi2020hmq,
  title={HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs},
  author={Habi, Hai Victor and Jennings, Roy H and Netzer, Arnon},
  journal={arXiv preprint arXiv:2007.09952},
  year={2020}
}


@article{linneural,
  title={Neural-Hardware Architecture Search},
  author={Lin, Yujun and Hafdi, Driss and Wang, Kuan and Liu, Zhijian and Han, Song},
  year={2020}
}


@article{choi2020dance,
  title={DANCE: Differentiable Accelerator/Network Co-Exploration},
  author={Choi, Kanghyun and Hong, Deokki and Yoon, Hojae and Yu, Joonsang and Kim, Youngsok and Lee, Jinho},
  journal={arXiv preprint arXiv:2009.06237},
  year={2020}
}

@inproceedings{guo2020single,
  title={Single path one-shot neural architecture search with uniform sampling},
  author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
  booktitle={European Conference on Computer Vision},
  pages={544--560},
  year={2020},
  organization={Springer}
}

@article{cai2019once,
  title={Once-for-all: Train one network and specialize it for efficient deployment},
  author={Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  journal={arXiv preprint arXiv:1908.09791},
  year={2019}
}

@article{yu2020bignas,
  title={Bignas: Scaling up neural architecture search with big single-stage models},
  author={Yu, Jiahui and Jin, Pengchong and Liu, Hanxiao and Bender, Gabriel and Kindermans, Pieter-Jan and Tan, Mingxing and Huang, Thomas and Song, Xiaodan and Pang, Ruoming and Le, Quoc},
  journal={arXiv preprint arXiv:2003.11142},
  year={2020}
}


@inproceedings{gao2017tetris,
  title={Tetris: Scalable and efficient neural network acceleration with 3d memory},
  author={Gao, Mingyu and Pu, Jing and Yang, Xuan and Horowitz, Mark and Kozyrakis, Christos},
  booktitle={Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={751--764},
  year={2017}
}


@inproceedings{real2019regularized,
  title={Regularized evolution for image classifier architecture search},
  author={Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={33},
  number={01},
  pages={4780--4789},
  year={2019}
}

@article{miller1995genetic,
  title={Genetic algorithms, tournament selection, and the effects of noise},
  author={Miller, Brad L and Goldberg, David E and others},
  journal={Complex systems},
  volume={9},
  number={3},
  pages={193--212},
  year={1995},
  publisher={[Champaign, IL, USA: Complex Systems Publications, Inc., c1987-}
}


@article{guo2019single,
  title={Single path one-shot neural architecture search with uniform sampling},
  author={Guo, Zichao and Zhang, Xiangyu and Mu, Haoyuan and Heng, Wen and Liu, Zechun and Wei, Yichen and Sun, Jian},
  journal={arXiv preprint arXiv:1904.00420},
  year={2019}
}



@inproceedings{wang2018interpret,
  title={Interpret neural networks by identifying critical data routing paths},
  author={Wang, Yulong and Su, Hang and Zhang, Bo and Hu, Xiaolin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8906--8914},
  year={2018}
}

@inproceedings{bender2018understanding,
  title={Understanding and simplifying one-shot architecture search},
  author={Bender, Gabriel and Kindermans, Pieter-Jan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc},
  booktitle={International Conference on Machine Learning},
  pages={550--559},
  year={2018}
}

@inproceedings{tan2021efficientnetv2,
  title={Efficientnetv2: Smaller models and faster training},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={10096--10106},
  year={2021},
  organization={PMLR}
}


@inproceedings{qiu2019adversarial,
  title={Adversarial defense through network profiling based path extraction},
  author={Qiu, Yuxian and Leng, Jingwen and Guo, Cong and Chen, Quan and Li, Chao and Guo, Minyi and Zhu, Yuhao},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4777--4786},
  year={2019}
}


@inproceedings{fu2021auto,
  title={Auto-NBA: Efficient and effective search over the joint space of networks, bitwidths, and accelerators},
  author={Fu, Yonggan and Zhang, Yongan and Zhang, Yang and Cox, David and Lin, Yingyan},
  booktitle={International Conference on Machine Learning},
  pages={3505--3517},
  year={2021},
  organization={PMLR}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}




@article{jiang2020standing,
  title={Standing on the shoulders of giants: Hardware and neural architecture co-search with hot start},
  author={Jiang, Weiwen and Yang, Lei and Dasgupta, Sakyasingha and Hu, Jingtong and Shi, Yiyu},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={39},
  number={11},
  pages={4154--4165},
  year={2020},
  publisher={IEEE}
}

@article{thakur2022benchmarking,
  title={Benchmarking Large Language Models for Automated Verilog RTL Code Generation},
  author={Thakur, Shailja and Ahmad, Baleegh and Fan, Zhenxing and Pearce, Hammond and Tan, Benjamin and Karri, Ramesh and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:2212.11140},
  year={2022}
}

@article{ahmad2023fixing,
  title={Fixing Hardware Security Bugs with Large Language Models},
  author={Ahmad, Baleegh and Thakur, Shailja and Tan, Benjamin and Karri, Ramesh and Pearce, Hammond},
  journal={arXiv preprint arXiv:2302.01215},
  year={2023}
}


@inproceedings{jia2021tensorlib,
  title={Tensorlib: A spatial accelerator generation framework for tensor algebra},
  author={Jia, Liancheng and Luo, Zizhang and Lu, Liqiang and Liang, Yun},
  booktitle={2021 58th ACM/IEEE Design Automation Conference (DAC)},
  pages={865--870},
  year={2021},
  organization={IEEE}
}

@inproceedings{zhang2018dnnbuilder,
  title={{DNNBuilder}: an automated tool for building high-performance {DNN} hardware accelerators for {FPGAs}},
  author={Zhang, Xiaofan and others},
  booktitle={Proceedings of the International Conference on Computer-Aided Design},
  pages={56},
  year={2018},
  organization={ACM}
}

@inproceedings{venkatesan2019magnet,
  title={Magnet: A modular accelerator generator for neural networks},
  author={Venkatesan, Rangharajan and Shao, Yakun Sophia and Wang, Miaorong and Clemons, Jason and Dai, Steve and Fojtik, Matthew and others},
  booktitle={2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

@article{garg2021taxonomy,
  title={A Taxonomy for Classification and Comparison of Dataflows for GNN Accelerators},
  author={Garg, Raveesh and others},
  journal={arXiv preprint arXiv:2103.07977},
  year={2021}
}

@inproceedings{liang2020deepburning,
  title={DeepBurning-GL: an automated framework for generating graph neural network accelerators},
  author={Liang, Shengwen and others},
  booktitle={ICCAD},
  year={2020},
}



@article{zheng2023codegeex,
  title={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X},
  author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and others},
  journal={arXiv preprint arXiv:2303.17568},
  year={2023}
}

@article{nijkamp2022codegen,
  title={Codegen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{koubaa2023gpt,
  title={GPT-4 vs. GPT-3.5: A concise showdown},
  author={Koubaa, Anis},
  year={2023},
  publisher={Preprints}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and others},
  journal={NeurIPS},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@article{floridi2020gpt,
  title={GPT-3: Its nature, scope, limits, and consequences},
  author={Floridi, Luciano and Chiriatti, Massimo},
  journal={Minds and Machines},
  volume={30},
  pages={681--694},
  year={2020},
  publisher={Springer}
}

@article{zhang2019dialogpt,
  title={Dialogpt: Large-scale generative pre-training for conversational response generation},
  author={Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, Jingjing and Dolan, Bill},
  journal={arXiv preprint arXiv:1911.00536},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{ji2023towards,
  title={Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation},
  author={Ji, Yunjie and Gong, Yan and Deng, Yong and Peng, Yiping and Niu, Qiang and Ma, Baochang and Li, Xiangang},
  journal={arXiv preprint arXiv:2304.07854},
  year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}



@INPROCEEDINGS{zhang2021dian,
  author={Zhang, Yongan and Fu, Yonggan and Jiang, Weiwen and Li, Chaojian and You, Haoran and Li, Meng and Chandra, Vikas and Lin, Yingyan},
  booktitle={2021 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)}, 
  title={DIAN: Differentiable Accelerator-Network Co-Search Towards Maximal DNN Efficiency}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ISLPED52811.2021.9502478}}


@INPROCEEDINGS{zhang2021gcos,
  author={Zhang, Yongan and You, Haoran and Fu, Yonggan and Geng, Tong and Li, Ang and Lin, Yingyan},
  booktitle={2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)}, 
  title={G-CoS: GNN-Accelerator Co-Search Towards Both Better Accuracy and Efficiency}, 
  year={2021},
  volume={},
  number={},
  pages={1-9},
  doi={10.1109/ICCAD51958.2021.9643549}}

@article{zhang2023llama,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang and others},
  journal={arXiv},
  year={2023}
}


@misc{MPSoC,
author = {},
title = {Ds891-zynq-ultrascale-plus-overview},
howpublished = {\url{https://docs.xilinx.com/v/u/en-US/ds891-zynq-ultrascale-plus-overview}},
month = {},
year = {},
note = {(Accessed on 02/21/2023)}
}
@misc{hlx,
author = {{Xilinx Inc.}},
title = {Vivado Design Suite},
howpublished = {\url{https://www.xilinx.com/products/design-tools/vivado.html}},
month = {},
year = {},
note = {(Accessed on 05/08/2023)}
}

@misc{gpt35,
author = {{OpenAI}},
title = {GPT-3.5},
howpublished = {\url{https://platform.openai.com/docs/models/gpt-3-5}},
month = {},
year = {},
note = {(Accessed on 04/10/2023)}
}

@misc{gpt4,
author = {{OpenAI}},
title = {GPT-4},
howpublished = {\url{https://platform.openai.com/docs/models/gpt-4}},
month = {},
year = {},
note = {(Accessed on 04/10/2023)}
}

@article{zhang2020skynet,
  title={SkyNet: a hardware-efficient method for object detection and tracking on embedded systems},
  author={Zhang, Xiaofan and Lu, Haoming and Hao, Cong and Li, Jiachen and Cheng, Bowen and Li, Yuhong and Rupnow, Kyle and and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={216--229},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{chan2022data,
  title={Data distributional properties drive emergent few-shot learning in transformers},
  author={Chan, Stephanie CY and Santoro, Adam and Lampinen, Andrew K and Wang, Jane X and Singh, Aaditya and Richemond, Pierre H and McClelland, Jay and Hill, Felix},
  journal={arXiv preprint arXiv:2205.05055},
  year={2022}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{razeghi2022impact,
  title={Impact of pretraining term frequencies on few-shot reasoning},
  author={Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:2202.07206},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and others},
  journal={NeurIPS},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{rong2021extrapolating,
  title={Extrapolating to unnatural language processing with gpt-3’s in-context learning: The good, the bad, and the mysterious},
  author={Rong, Frieda},
  year={2021}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{min2021noisy,
  title={Noisy channel language model prompting for few-shot text classification},
  author={Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2108.04106},
  year={2021}
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning},
  pages={12697--12706},
  year={2021},
  organization={PMLR}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={NeurIPS},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@article{zhang2022active,
  title={Active example selection for in-context learning},
  author={Zhang, Yiming and Feng, Shi and Tan, Chenhao},
  journal={arXiv preprint arXiv:2211.04486},
  year={2022}
}

@article{qin2022enabling,
  title={Enabling flexibility for sparse tensor acceleration via heterogeneity},
  author={Qin, Eric and Garg, Raveesh and Bambhaniya, Abhimanyu and Pellauer, Michael and Parashar, Angshuman and Rajamanickam, Sivasankaran and Hao, Cong and Krishna, Tushar},
  journal={arXiv preprint arXiv:2201.08916},
  year={2022}
}

@article{wei2023hlsdataset,
  title={HLSDataset: Open-Source Dataset for ML-Assisted FPGA Design using High Level Synthesis},
  author={Wei, Zhigang and Arora, Aman and John, Lizy K},
  journal={arXiv preprint arXiv:2302.10977},
  year={2023}
}


@article{thakur2022benchmarking,
  title={Benchmarking Large Language Models for Automated Verilog RTL Code Generation},
  author={Thakur, Shailja and Ahmad, Baleegh and Fan, Zhenxing and Pearce, Hammond and Tan, Benjamin and Karri, Ramesh and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:2212.11140},
  year={2022}
}


@article{wang2022code4struct,
  title={Code4struct: Code generation for few-shot structured prediction from natural language},
  author={Wang, Xingyao and Li, Sha and Ji, Heng},
  journal={arXiv preprint arXiv:2210.12810},
  year={2022}
}

@inproceedings{khailany2018modular,
  title={A modular digital VLSI flow for high-productivity SoC design},
  author={Khailany, Brucek and Venkatesan, Rangharajan and Clemons, Jason and Emer, Joel S and Fojtik, Matthew and Klinefelter, Alicia and others},
  booktitle={Proceedings of the 55th Annual Design Automation Conference},
  pages={1--6},
  year={2018}
}


@article{poesia2022synchromesh,
  title={Synchromesh: Reliable code generation from pre-trained language models},
  author={Poesia, Gabriel and Polozov, Oleksandr and Le, Vu and Tiwari, Ashish and Soares, Gustavo and Meek, Christopher and Gulwani, Sumit},
  journal={arXiv preprint arXiv:2201.11227},
  year={2022}
}

@article{chen2023improving,
  title={Improving Code Generation by Training with Natural Language Feedback},
  author={Chen, Angelica and Scheurer, J{\'e}r{\'e}my and Korbak, Tomasz and Campos, Jon Ander and Chan, Jun Shern and Bowman, Samuel R and Cho, Kyunghyun and Perez, Ethan},
  journal={arXiv preprint arXiv:2303.16749},
  year={2023}
}



@article{zhang2020skynet,
  title={SkyNet: a hardware-efficient method for object detection and tracking on embedded systems},
  author={Zhang, Xiaofan and Lu, Haoming and Hao, Cong and Li, Jiachen and Cheng, Bowen and Li, Yuhong and Rupnow, Kyle and Xiong, Jinjun and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={216--229},
  year={2020}
}


@inproceedings{khailany2018modular,
  title={A modular digital VLSI flow for high-productivity SoC design},
  author={Khailany, Brucek and Venkatesan, Rangharajan and Clemons, Jason and Emer, Joel S and Fojtik, Matthew and Klinefelter, Alicia and others},
  booktitle={Proceedings of the 55th Annual Design Automation Conference},
  pages={1--6},
  year={2018}
}


@misc{chaidnn_v2,
    author={{Xilinx Inc.}},
    title={{Chaidnnv2 - hls based dnn accelerator library for xilinx ultrascale+ mpsocs}},
    note={\url{https://github.com/Xilinx/CHaiDNN}, accessed 2023-05-08},
}


@misc{pynq,
    author={{Xilinx Inc.}},
    title={{PYNQ: PYTHON PRODUCTIVITY}},
    note={\url{http://www.pynq.io/}, accessed 2023-05-08},
}

@inproceedings{kim2023range,
  title={Range-Invariant Approximation of Non-Linear Operations for Efficient BERT Fine-Tuning},
  author={Kim, Janghyeon and Lee, Janghwan and Choi, Jungwook and Han, JeongHo and Lee, Sangheon},
  booktitle={2023 60th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}

@inproceedings{tambe2021edgebert,
  title={Edgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference},
  author={Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and others},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={830--844},
  year={2021}
}

@inproceedings{tambe202322,
  title={22.9 A 12nm 18.1 TFLOPs/W Sparse Transformer Processor with Entropy-Based Early Exit, Mixed-Precision Predication and Fine-Grained Power Management},
  author={Tambe, Thierry and Zhang, Jeff and Hooper, Coleman and Jia, Tianyu and Whatmough, Paul N and Zuckerman, Joseph and Dos Santos, Maico Cassel and others},
  booktitle={2023 IEEE International Solid-State Circuits Conference (ISSCC)},
  pages={342--344},
  year={2023},
  organization={IEEE}
}

@article{xin2020deebert,
  title={DeeBERT: Dynamic early exiting for accelerating BERT inference},
  author={Xin, Ji and Tang, Raphael and Lee, Jaejun and Yu, Yaoliang and Lin, Jimmy},
  journal={arXiv preprint arXiv:2004.12993},
  year={2020}
}

@article{zhou2020bert,
  title={Bert loses patience: Fast and robust inference with early exit},
  author={Zhou, Wangchunshu and Xu, Canwen and Ge, Tao and McAuley, Julian and Xu, Ke and Wei, Furu},
  journal={NeurIPS},
  volume={33},
  pages={18330--18341},
  year={2020}
}

@article{li2021accelerating,
  title={Accelerating bert inference for sequence labeling via early-exit},
  author={Li, Xiaonan and Shao, Yunfan and Sun, Tianxiang and Yan, Hang and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2105.13878},
  year={2021}
}

@article{del2023skipdecode,
  title={SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference},
  author={Del Corro, Luciano and Del Giorno, Allie and Agarwal, Sahaj and Yu, Bin and Awadallah, Ahmed and Mukherjee, Subhabrata},
  journal={arXiv preprint arXiv:2307.02628},
  year={2023}
}

@article{schuster2022confident,
  title={Confident adaptive language modeling},
  author={Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh and Tay, Yi and Metzler, Donald},
  journal={NeurIPS},
  volume={35},
  pages={17456--17472},
  year={2022}
}

@article{schuster2021consistent,
  title={Consistent accelerated inference via confident adaptive transformers},
  author={Schuster, Tal and Fisch, Adam and Jaakkola, Tommi and Barzilay, Regina},
  journal={arXiv preprint arXiv:2104.08803},
  year={2021}
}

@inproceedings{xin2021berxit,
  title={BERxiT: Early exiting for BERT with better fine-tuning and extension to regression},
  author={Xin, Ji and Tang, Raphael and Yu, Yaoliang and Lin, Jimmy},
  booktitle={Proceedings of the 16th conference of the European chapter of the association for computational linguistics: Main Volume},
  pages={91--104},
  year={2021}
}

@inproceedings{shen2020q,
  title={Q-bert: Hessian based ultra low precision quantization of bert},
  author={Shen and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8815--8821},
  year={2020}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao and others},
  journal={NeurIPS},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{thirunavukarasu2023large,
  title={Large language models in medicine},
  author={Thirunavukarasu and others},
  journal={Nature medicine},
  volume={29},
  number={8},
  pages={1930--1940},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@article{safdari2023personality,
  title={Personality traits in large language models},
  author={Safdari and others},
  journal={arXiv},
  year={2023}
}

@article{xi2023rise,
  title={The rise and potential of large language model based agents: A survey},
  author={Xi and others},
  journal={arXiv},
  year={2023}
}

@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers and others},
  journal={arXiv},
  year={2023}
}

@article{liu2023llm,
  title={LLM-QAT: Data-Free Quantization Aware Training for Large Language Models},
  author={Liu and others},
  journal={arXiv},
  year={2023}
}

@article{sung2022lst,
  title={Lst: Ladder side-tuning for parameter and memory efficient transfer learning},
  author={Sung  and others},
  journal={NeurIPS},
  volume={35},
  pages={12991--13005},
  year={2022}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu and others},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{frantar2023sparsegpt,
  title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Frantar and others},
  year={2023}
}

@misc{taori2023stanford,
  title={Stanford alpaca: An instruction-following llama model},
  author={Taori and others},
  year={2023}
}

@inproceedings{teerapittayanon2016branchynet,
  title={Branchynet: Fast inference via early exiting from deep neural networks},
  author={Teerapittayanon and others},
  booktitle={ICPR},
  year={2016},
  
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks and others},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity and others},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{shao2023efficient,
  title={An Efficient Training Accelerator for Transformers With Hardware-Algorithm Co-Optimization},
  author={Shao and others},
  journal={VLSI},
  year={2023},
  publisher={IEEE}
}


@misc{tx2,
    author={{NVIDIA}},
    title={{NVIDIA Jetson TX2}},
    note={\url{www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-tx2/}},
    year={2020},
}

\bibitem{SCALE-Sim} A. Samajdar, Y. Zhu, and P. Whatmough, ``Systolic CNN AcceLErator Simulator (SCALE Sim)'' 

@misc{scalesim,
    author={{Samajdar et al}},
    title={{Systolic CNN AcceLErator Simulator (SCALE Sim)}},
    note={\url{https://github.com/ARM-software/SCALE-Sim}},
    year={2023},
}

@misc{quest,
    author={{Meta}},
    Title={{Quest Pro}},
    note={\url{https://www.meta.com/quest/quest-pro/}},
    year={2022},
}

@inproceedings{fu20212,
  title={Enabling random precision switch for winning both adversarial robustness and efficiency},
  author={Fu and others},
  booktitle={MICRO},
  pages={225--237},
  year={2021}
}

@article{kim2024memory,
  title={Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization},
  author={Kim et al},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@inproceedings{yu2023hint,
  title={Hint-aug: Drawing hints from foundation vision transformers towards boosted few-shot parameter-efficient tuning},
  author={Yu et al},
  booktitle={CVPR},
  pages={11102--11112},
  year={2023}
}

@inproceedings{yu2023master,
  title={Master-ASR: achieving multilingual scalability and low-resource adaptation in ASR with modular learning},
  author={Yu et al},
  booktitle={ICML},
  pages={40475--40487},
  year={2023},
  organization={PMLR}
}

@article{yu2022unified,
  title={Unified visual transformer compression},
  author={Yu et al},
  journal={arXiv preprint arXiv:2203.08243},
  year={2022}
}


@article{pearce2021understanding,
  title={Understanding softmax confidence and uncertainty},
  author={Pearce and others},
  journal={arXiv preprint arXiv:2106.04972},
  year={2021}
}